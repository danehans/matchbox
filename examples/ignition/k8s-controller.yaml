---
networkd:
  units:
    - name: 10-int.network
      contents: |
        [Match]
        Name={{.int_intf}}

        [Network]
        DHCP=true
        NTP={{.ntp_server}}

        [DHCP]
        UseRoutes=false
    - name: 20-ext.network
      contents: |
        [Match]
        Name={{.ext_intf}}
        [Network]
        Address={{.ext_ip_address}}
        Gateway={{.ext_gateway}}

systemd:
  units:
    - name: etcd-member.service
      enable: true
      dropins:
        - name: 40-etcd-cluster.conf
          contents: |
            [Service]
            Environment="ETCD_IMAGE_TAG=v3.1.5"
            Environment="ETCD_NAME={{.etcd_name}}"
            Environment="ETCD_ADVERTISE_CLIENT_URLS=http://{{.domain_name}}:2379"
            Environment="ETCD_INITIAL_ADVERTISE_PEER_URLS=http://{{.domain_name}}:2380"
            Environment="ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379"
            Environment="ETCD_LISTEN_PEER_URLS=http://0.0.0.0:2380"
            Environment="ETCD_INITIAL_CLUSTER={{.etcd_initial_cluster}}"
            Environment="ETCD_STRICT_RECONFIG_CHECK=true"
    - name: locksmithd.service
      dropins:
        - name: 40-etcd-lock.conf
          contents: |
            [Service]
            Environment="REBOOT_STRATEGY=etcd-lock"
    - name: k8s-certs@.service
      contents: |
        [Unit]
        Description=Fetch Kubernetes certificate assets
        Requires=network-online.target
        After=network-online.target
        [Service]
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/ssl
        ExecStart=/usr/bin/bash -c "[ -f /etc/kubernetes/ssl/%i ] || curl {{.k8s_cert_endpoint}}/tls/%i -o /etc/kubernetes/ssl/%i"
    - name: k8s-assets.target
      contents: |
        [Unit]
        Description=Load Kubernetes Assets
        Requires=k8s-certs@apiserver.pem.service
        After=k8s-certs@apiserver.pem.service
        Requires=k8s-certs@apiserver-key.pem.service
        After=k8s-certs@apiserver-key.pem.service
        Requires=k8s-certs@ca.pem.service
        After=k8s-certs@ca.pem.service
    - name: kubelet.service
      enable: true
      contents: |
        [Unit]
        Description=Kubelet via Hyperkube ACI
        Requires=k8s-assets.target
        After=k8s-assets.target
        [Service]
        Environment=KUBELET_VERSION=v1.6.1_coreos.0
        Environment="RKT_OPTS=--uuid-file-save=/var/run/kubelet-pod.uuid \
          --volume dns,kind=host,source=/etc/resolv.conf \
          --mount volume=dns,target=/etc/resolv.conf \
          {{ if eq .container_runtime "rkt" -}}
          --volume rkt,kind=host,source=/opt/bin/host-rkt \
          --mount volume=rkt,target=/usr/bin/rkt \
          --volume var-lib-rkt,kind=host,source=/var/lib/rkt \
          --mount volume=var-lib-rkt,target=/var/lib/rkt \
          --volume stage,kind=host,source=/tmp \
          --mount volume=stage,target=/tmp \
          {{ end -}}
          --volume var-log,kind=host,source=/var/log \
          --mount volume=var-log,target=/var/log"
          --volume cni-bin,kind=host,source=/opt/cni/bin \
          --mount volume=cni-bin,target=/opt/cni/bin
        ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
          --api-servers=http://127.0.0.1:8080 \
          --register-schedulable=true \
          --cni-conf-dir=/etc/kubernetes/cni/net.d \
          --network-plugin=cni \
          --container-runtime={{.container_runtime}} \
          --rkt-path=/usr/bin/rkt \
          --rkt-stage1-image=coreos.com/rkt/stage1-coreos \
          --allow-privileged=true \
          --pod-manifest-path=/etc/kubernetes/manifests \
          --hostname-override={{.domain_name}} \
          --cluster_dns={{.k8s_dns_service_ip}} \
          --cluster_domain=cluster.local
        ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
        Restart=always
        RestartSec=10
        [Install]
        WantedBy=multi-user.target
    - name: k8s-addons.service
      enable: true
      contents: |
        [Unit]
        Description=Kubernetes Addons
        [Service]
        Type=oneshot
        ExecStart=/opt/k8s-addons
        [Install]
        WantedBy=multi-user.target
    {{ if eq .container_runtime "rkt" }}
    - name: rkt-api.service
      enable: true
      contents: |
        [Unit]
        Before=kubelet.service
        [Service]
        ExecStart=/usr/bin/rkt api-service
        Restart=always
        RestartSec=10
        [Install]
        RequiredBy=kubelet.service
    - name: load-rkt-stage1.service
      enable: true
      contents: |
        [Unit]
        Description=Load rkt stage1 images
        Documentation=http://github.com/coreos/rkt
        Requires=network-online.target
        After=network-online.target
        Before=rkt-api.service
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/usr/bin/rkt fetch /usr/lib/rkt/stage1-images/stage1-coreos.aci /usr/lib/rkt/stage1-images/stage1-fly.aci  --insecure-options=image
        [Install]
        RequiredBy=rkt-api.service
    {{ end }}

storage:
  {{ if index . "pxe" }}
  disks:
    - device: /dev/sda
      wipe_table: true
      partitions:
        - label: ROOT
  filesystems:
    - name: root
      mount:
        device: "/dev/sda1"
        format: "ext4"
        create:
          force: true
          options:
            - "-LROOT"
  {{ end }}
  files:
    - path: /etc/sysctl.d/max-user-watches.conf
      filesystem: root
      contents:
        inline: |
          fs.inotify.max_user_watches=16184
    - path: /etc/kubernetes/manifests/kube-proxy.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: Pod
          metadata:
            name: kube-proxy
            namespace: kube-system
            annotations:
              rkt.alpha.kubernetes.io/stage1-name-override: coreos.com/rkt/stage1-fly
          spec:
            hostNetwork: true
            containers:
            - name: kube-proxy
              image: quay.io/coreos/hyperkube:v1.6.1_coreos.0
              command:
              - /hyperkube
              - proxy
              - --master=http://127.0.0.1:8080
              securityContext:
                privileged: true
              volumeMounts:
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
              - mountPath: /var/run/dbus
                name: dbus
                readOnly: false
            volumes:
            - hostPath:
                path: /usr/share/ca-certificates
              name: ssl-certs-host
            - hostPath:
                path: /var/run/dbus
              name: dbus
    - path: /etc/kubernetes/manifests/kube-apiserver.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: Pod
          metadata:
            name: kube-apiserver
            namespace: kube-system
          spec:
            hostNetwork: true
            containers:
            - name: kube-apiserver
              image: quay.io/coreos/hyperkube:v1.6.1_coreos.0
              command:
              - /hyperkube
              - apiserver
              - --bind-address=0.0.0.0
              - --storage-backend=etcd3
              - --etcd-servers={{.k8s_etcd_endpoints}}
              - --allow-privileged=true
              - --service-cluster-ip-range={{.k8s_service_ip_range}}
              - --secure-port=443
              - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
              - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
              - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
              - --client-ca-file=/etc/kubernetes/ssl/ca.pem
              - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
              - --runtime-config=extensions/v1beta1/networkpolicies=true
              - --anonymous-auth=false
              livenessProbe:
                httpGet:
                  host: 127.0.0.1
                  port: 8080
                  path: /healthz
                initialDelaySeconds: 15
                timeoutSeconds: 15
              ports:
              - containerPort: 443
                hostPort: 443
                name: https
              - containerPort: 8080
                hostPort: 8080
                name: local
              volumeMounts:
              - mountPath: /etc/kubernetes/ssl
                name: ssl-certs-kubernetes
                readOnly: true
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
            volumes:
            - hostPath:
                path: /etc/kubernetes/ssl
              name: ssl-certs-kubernetes
            - hostPath:
                path: /usr/share/ca-certificates
              name: ssl-certs-host
    - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: Pod
          metadata:
            name: kube-controller-manager
            namespace: kube-system
          spec:
            containers:
            - name: kube-controller-manager
              image: quay.io/coreos/hyperkube:v1.6.1_coreos.0
              command:
              - /hyperkube
              - controller-manager
              - --master=http://127.0.0.1:8080
              - --leader-elect=true
              - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
              - --root-ca-file=/etc/kubernetes/ssl/ca.pem
              resources:
                requests:
                  cpu: 200m
              livenessProbe:
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10252
                initialDelaySeconds: 15
                timeoutSeconds: 15
              volumeMounts:
              - mountPath: /etc/kubernetes/ssl
                name: ssl-certs-kubernetes
                readOnly: true
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
            hostNetwork: true
            volumes:
            - hostPath:
                path: /etc/kubernetes/ssl
              name: ssl-certs-kubernetes
            - hostPath:
                path: /usr/share/ca-certificates
              name: ssl-certs-host
    - path: /etc/kubernetes/manifests/kube-scheduler.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: Pod
          metadata:
            name: kube-scheduler
            namespace: kube-system
          spec:
            hostNetwork: true
            containers:
            - name: kube-scheduler
              image: quay.io/coreos/hyperkube:v1.6.1_coreos.0
              command:
              - /hyperkube
              - scheduler
              - --master=http://127.0.0.1:8080
              - --leader-elect=true
              resources:
                requests:
                  cpu: 100m
              livenessProbe:
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10251
                initialDelaySeconds: 15
                timeoutSeconds: 15
    - path: /etc/kubernetes/apiserver-parrotconfig.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: Config
          clusters:
          - name: local
            cluster:
              server: "http://127.0.0.1:8080"
          users:
          - name: parrot
          contexts:
          - context:
              cluster: local
              user: parrot
            name: parrot-context
          current-context: parrot-context
    - path: /etc/kubernetes/manifests/kube-parrot.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: Pod
          metadata:
            name: kube-parrot
            namespace: kube-system
          spec:
            hostNetwork: true
            volumes:
              - name: etc-kubernetes
                hostPath:
                  path: /etc/kubernetes
            containers:
              - name: parrot
                image: sapcc/kube-parrot:v201611131942
                volumeMounts:
                  - mountPath: /etc/kubernetes
                    name: etc-kubernetes
                    readOnly: true
                args:
                  - --local_address={{.int_ip_address}}
                  - --master_address=127.0.0.1
                  - --service_subnet={{.k8s_service_ip_range}}
                  - --as={{.bgp_as}}
                  - --logtostderr
                  - --v=5
                  - --kubeconfig=/etc/kubernetes/apiserver-parrotconfig.yaml{{ range $neighbor := .bgp_neighbors }}
                  - --neighbor={{$neighbor}}{{end}}
    - path: /srv/kubernetes/manifests/netplugin-crb.yaml
      filesystem: root
      contents:
        inline: |
          kind: ClusterRoleBinding
          apiVersion: rbac.authorization.k8s.io/v1beta1
          metadata:
            name: contiv-netplugin
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: contiv-netplugin
          subjects:
          - kind: ServiceAccount
            name: contiv-netplugin
            namespace: kube-system
    - path: /srv/kubernetes/manifests/netplugin-cr.yaml
      filesystem: root
      contents:
        inline: |
          kind: ClusterRole
          apiVersion: rbac.authorization.k8s.io/v1beta1
          metadata:
            name: contiv-netplugin
            namespace: kube-system
          rules:
            - apiGroups:
                - ""
              resources:
                - pods
                - nodes
              verbs:
                - get
    - path: /srv/kubernetes/manifests/netplugin-sa.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: contiv-netplugin
            namespace: kube-system
    - path: /srv/kubernetes/manifests/netmaster-crb.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: rbac.authorization.k8s.io/v1beta1
          kind: ClusterRoleBinding
          metadata:
            name: contiv-netmaster
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: contiv-netmaster
          subjects:
          - kind: ServiceAccount
            name: contiv-netmaster
            namespace: kube-system
    - path: /srv/kubernetes/manifests/netmaster-cr.yaml
      filesystem: root
      contents:
        inline: |
          kind: ClusterRole
          apiVersion: rbac.authorization.k8s.io/v1beta1
          metadata:
            name: contiv-netmaster
            namespace: kube-system
          rules:
            - apiGroups:
              - ""
              - extensions
              resources:
                - pods
                - nodes
                - namespaces
                - networkpolicies
              verbs:
                - watch
                - list
                - update
    - path: /srv/kubernetes/manifests/netmaster-sa.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: contiv-netmaster
            namespace: kube-system
    - path: /srv/kubernetes/manifests/contiv-cm.yaml
      filesystem: root
      contents:
        inline: |
          kind: ConfigMap
          apiVersion: v1
          metadata:
            name: contiv-config
            namespace: kube-system
          data:
            cluster_store: "etcd://10.10.129.135:2379"
            cni_config: |-
              {
                "cniVersion": "0.1.0",
                "name": "contiv-net",
                "type": "contivk8s"
              }
            config: |-
              {
                 "K8S_API_SERVER": "https://__NETMASTER_IP__:6443",
                 "K8S_CA": "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt",
                 "K8S_KEY": "",
                 "K8S_CERT": "",
                 "K8S_TOKEN": ""
              }
    - path: /srv/kubernetes/manifests/contiv-ds.yaml
      filesystem: root
      contents:
        inline: |
          kind: DaemonSet
          apiVersion: extensions/v1beta1
          metadata:
            name: contiv-netplugin
            namespace: kube-system
            labels:
              k8s-app: contiv-netplugin
          spec:
            selector:
              matchLabels:
                k8s-app: contiv-netplugin
            template:
              metadata:
                labels:
                  k8s-app: contiv-netplugin
                annotations:
                  scheduler.alpha.kubernetes.io/critical-pod: ''
              spec:
                hostNetwork: true
                hostPID: true
                tolerations:
                - key: node-role.kubernetes.io/master
                  effect: NoSchedule
                serviceAccountName: contiv-netplugin
                containers:
                  - name: contiv-netplugin
                    image: contiv/netplugin:1.0.0
                    args:
                      - -pkubernetes
                      - -x
                    env:
                      - name: VLAN_IF
                        value: __VLAN_IF__
                      - name: VTEP_IP
                        valueFrom:
                           fieldRef:
                              fieldPath: status.podIP
                      - name: CONTIV_ETCD
                        valueFrom:
                          configMapKeyRef:
                            name: contiv-config
                            key: cluster_store
                      - name: CONTIV_CNI_CONFIG
                        valueFrom:
                          configMapKeyRef:
                            name: contiv-config
                            key: cni_config
                      - name: CONTIV_CONFIG
                        valueFrom:
                          configMapKeyRef:
                            name: contiv-config
                            key: config
                    securityContext:
                      privileged: true
                    volumeMounts:
                      - mountPath: /etc/openvswitch
                        name: etc-openvswitch
                        readOnly: false
                      - mountPath: /lib/modules
                        name: lib-modules
                        readOnly: false
                      - mountPath: /var/run
                        name: var-run
                        readOnly: false
                      - mountPath: /var/contiv
                        name: var-contiv
                        readOnly: false
                      - mountPath: /etc/kubernetes/pki
                        name: etc-kubernetes-pki
                        readOnly: false
                      - mountPath: /etc/kubernetes/ssl
                        name: etc-kubernetes-ssl
                        readOnly: false
                      - mountPath: /opt/cni/bin
                        name: cni-bin-dir
                        readOnly: false
                      - mountPath: /etc/cni/net.d/
                        name: etc-cni-dir
                        readOnly: false
                volumes:
                  - name: etc-openvswitch
                    hostPath:
                      path: /etc/openvswitch
                  - name: lib-modules
                    hostPath:
                      path: /lib/modules
                  - name: var-run
                    hostPath:
                      path: /var/run
                  - name: var-contiv
                    hostPath:
                      path: /var/contiv
                  - name: etc-kubernetes-pki
                    hostPath:
                      path: /etc/kubernetes/pki
                  - name: etc-kubernetes-ssl
                    hostPath:
                      path: /etc/kubernetes/ssl
                  - name: cni-bin-dir
                    hostPath:
                      path: /opt/cni/bin
                  - name: etc-cni-dir
                    hostPath:
                      path: /etc/kubernetes/cni/net.d
    - path: /srv/kubernetes/manifests/netmaster-rs.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: extensions/v1beta1
          kind: ReplicaSet
          metadata:
            name: contiv-netmaster
            namespace: kube-system
            labels:
              k8s-app: contiv-netmaster
          spec:
            replicas: 1
            template:
              metadata:
                name: contiv-netmaster
                namespace: kube-system
                labels:
                  k8s-app: contiv-netmaster
                annotations:
                  scheduler.alpha.kubernetes.io/critical-pod: ''
              spec:
                hostNetwork: true
                hostPID: true
                tolerations:
                - key: node-role.kubernetes.io/master
                  effect: NoSchedule
                serviceAccountName: contiv-netmaster
                containers:
                  - name: contiv-netmaster
                    image: contiv/netplugin:1.0.0
                    args:
                      - -m
                      - -pkubernetes
                    env:
                      - name: CONTIV_ETCD
                        valueFrom:
                          configMapKeyRef:
                            name: contiv-config
                            key: cluster_store
                      - name: CONTIV_CONFIG
                        valueFrom:
                          configMapKeyRef:
                            name: contiv-config
                            key: config
                    securityContext:
                      privileged: true
                    volumeMounts:
                      - mountPath: /etc/openvswitch
                        name: etc-openvswitch
                        readOnly: false
                      - mountPath: /lib/modules
                        name: lib-modules
                        readOnly: false
                      - mountPath: /var/run
                        name: var-run
                        readOnly: false
                      - mountPath: /var/contiv
                        name: var-contiv
                        readOnly: false
                      - mountPath: /etc/kubernetes/ssl
                        name: etc-kubernetes-ssl
                        readOnly: false
                      - mountPath: /opt/cni/bin
                        name: cni-bin-dir
                        readOnly: false
                volumes:
                  - name: etc-openvswitch
                    hostPath:
                      path: /etc/openvswitch
                  - name: lib-modules
                    hostPath:
                      path: /lib/modules
                  - name: var-run
                    hostPath:
                      path: /var/run
                  - name: var-contiv
                    hostPath:
                      path: /var/contiv
                  - name: etc-kubernetes-ssl
                    hostPath:
                      path: /etc/kubernetes/ssl
                  - name: cni-bin-dir
                    hostPath:
                      path: /opt/cni/bin
    - path: /srv/kubernetes/manifests/contiv-proxy-rs.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: extensions/v1beta1
          kind: ReplicaSet
          metadata:
            name: contiv-api-proxy
            namespace: kube-system
            labels:
              k8s-app: contiv-api-proxy
          spec:
            replicas: 1
            template:
              metadata:
                name: contiv-api-proxy
                namespace: kube-system
                labels:
                  k8s-app: contiv-api-proxy
                annotations:
                  scheduler.alpha.kubernetes.io/critical-pod: ''
              spec:
                hostNetwork: true
                hostPID: true
                tolerations:
                - key: node-role.kubernetes.io/master
                  effect: NoSchedule
                serviceAccountName: contiv-netmaster
                containers:
                  - name: contiv-api-proxy
                    image: contiv/auth_proxy:1.0.0
                    args:
                      - --tls-key-file=/etc/kubernetes/ssl/apiserver-key.pem
                      - --tls-certificate=/etc/kubernetes/ssl/apiserver.pem
                      - --data-store-address=etcd://10.10.129.135:2379
                      - --netmaster-address=10.10.129.135:9999
                    env:
                      - name: NO_NETMASTER_STARTUP_CHECK
                        value: "0"
                      - name: CONTIV_ETCD
                        valueFrom:
                          configMapKeyRef:
                            name: contiv-config
                            key: cluster_store
                    securityContext:
                      privileged: false
                    volumeMounts:
                      - mountPath: /var/contiv
                        name: var-contiv
                        readOnly: false
                      - mountPath: /etc/ssl/certs
                        name: ssl-certs-host
                        readOnly: true
                      - mountPath: /etc/kubernetes/ssl
                        name: ssl-certs-kubernetes
                        readOnly: true
                volumes:
                  - name: var-contiv
                    hostPath:
                      path: /var/contiv
                  - hostPath:
                      path: /usr/share/ca-certificates
                    name: ssl-certs-host
                  - hostPath:
                      path: /etc/kubernetes/ssl
                    name: ssl-certs-kubernetes
    - path: /srv/kubernetes/manifests/kube-dns-autoscaler-deployment.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: extensions/v1beta1
          kind: Deployment
          metadata:
            name: kube-dns-autoscaler
            namespace: kube-system
            labels:
              k8s-app: kube-dns-autoscaler
              kubernetes.io/cluster-service: "true"
          spec:
            template:
              metadata:
                labels:
                  k8s-app: kube-dns-autoscaler
                annotations:
                  scheduler.alpha.kubernetes.io/critical-pod: ''
                  scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
              spec:
                containers:
                - name: autoscaler
                  image: gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.0.0
                  resources:
                      requests:
                          cpu: "20m"
                          memory: "10Mi"
                  command:
                    - /cluster-proportional-autoscaler
                    - --namespace=kube-system
                    - --configmap=kube-dns-autoscaler
                    - --mode=linear
                    - --target=Deployment/kube-dns
                    - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"min":1}}
                    - --logtostderr=true
                    - --v=2
    - path: /srv/kubernetes/manifests/kube-dns-deployment.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: extensions/v1beta1
          kind: Deployment
          metadata:
            name: kube-dns
            namespace: kube-system
            labels:
              k8s-app: kube-dns
              kubernetes.io/cluster-service: "true"
          spec:
            strategy:
              rollingUpdate:
                maxSurge: 10%
                maxUnavailable: 0
            selector:
              matchLabels:
                k8s-app: kube-dns
            template:
              metadata:
                labels:
                  k8s-app: kube-dns
                annotations:
                  scheduler.alpha.kubernetes.io/critical-pod: ''
                  scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
              spec:
                containers:
                - name: kubedns
                  image: gcr.io/google_containers/kubedns-amd64:1.9
                  resources:
                    limits:
                      memory: 170Mi
                    requests:
                      cpu: 100m
                      memory: 70Mi
                  livenessProbe:
                    httpGet:
                      path: /healthz-kubedns
                      port: 8080
                      scheme: HTTP
                    initialDelaySeconds: 60
                    timeoutSeconds: 5
                    successThreshold: 1
                    failureThreshold: 5
                  readinessProbe:
                    httpGet:
                      path: /readiness
                      port: 8081
                      scheme: HTTP
                    initialDelaySeconds: 3
                    timeoutSeconds: 5
                  args:
                  - --domain=cluster.local.
                  - --dns-port=10053
                  - --config-map=kube-dns
                  - --v=2
                  env:
                  - name: PROMETHEUS_PORT
                    value: "10055"
                  ports:
                  - containerPort: 10053
                    name: dns-local
                    protocol: UDP
                  - containerPort: 10053
                    name: dns-tcp-local
                    protocol: TCP
                  - containerPort: 10055
                    name: metrics
                    protocol: TCP
                - name: dnsmasq
                  image: gcr.io/google_containers/kube-dnsmasq-amd64:1.4
                  livenessProbe:
                    httpGet:
                      path: /healthz-dnsmasq
                      port: 8080
                      scheme: HTTP
                    initialDelaySeconds: 60
                    timeoutSeconds: 5
                    successThreshold: 1
                    failureThreshold: 5
                  args:
                  - --cache-size=1000
                  - --no-resolv
                  - --server=127.0.0.1#10053
                  - --log-facility=-
                  ports:
                  - containerPort: 53
                    name: dns
                    protocol: UDP
                  - containerPort: 53
                    name: dns-tcp
                    protocol: TCP
                  resources:
                    requests:
                      cpu: 150m
                      memory: 10Mi
                - name: dnsmasq-metrics
                  image: gcr.io/google_containers/dnsmasq-metrics-amd64:1.0
                  livenessProbe:
                    httpGet:
                      path: /metrics
                      port: 10054
                      scheme: HTTP
                    initialDelaySeconds: 60
                    timeoutSeconds: 5
                    successThreshold: 1
                    failureThreshold: 5
                  args:
                  - --v=2
                  - --logtostderr
                  ports:
                  - containerPort: 10054
                    name: metrics
                    protocol: TCP
                  resources:
                    requests:
                      memory: 10Mi
                - name: healthz
                  image: gcr.io/google_containers/exechealthz-amd64:1.2
                  resources:
                    limits:
                      memory: 50Mi
                    requests:
                      cpu: 10m
                      memory: 50Mi
                  args:
                  - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 >/dev/null
                  - --url=/healthz-dnsmasq
                  - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
                  - --url=/healthz-kubedns
                  - --port=8080
                  - --quiet
                  ports:
                  - containerPort: 8080
                    protocol: TCP
                dnsPolicy: Default
    - path: /srv/kubernetes/manifests/kube-dns-svc.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: Service
          metadata:
            name: kube-dns
            namespace: kube-system
            labels:
              k8s-app: kube-dns
              kubernetes.io/cluster-service: "true"
              kubernetes.io/name: "KubeDNS"
          spec:
            selector:
              k8s-app: kube-dns
            clusterIP: {{.k8s_dns_service_ip}}
            ports:
            - name: dns
              port: 53
              protocol: UDP
            - name: dns-tcp
              port: 53
              protocol: TCP
    - path: /srv/kubernetes/manifests/heapster-deployment.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: extensions/v1beta1
          kind: Deployment
          metadata:
            name: heapster-v1.2.0
            namespace: kube-system
            labels:
              k8s-app: heapster
              kubernetes.io/cluster-service: "true"
              version: v1.2.0
          spec:
            replicas: 1
            selector:
              matchLabels:
                k8s-app: heapster
                version: v1.2.0
            template:
              metadata:
                labels:
                  k8s-app: heapster
                  version: v1.2.0
                annotations:
                  scheduler.alpha.kubernetes.io/critical-pod: ''
                  scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
              spec:
                containers:
                  - image: gcr.io/google_containers/heapster:v1.2.0
                    name: heapster
                    livenessProbe:
                      httpGet:
                        path: /healthz
                        port: 8082
                        scheme: HTTP
                      initialDelaySeconds: 180
                      timeoutSeconds: 5
                    command:
                      - /heapster
                      - --source=kubernetes.summary_api:''
                  - image: gcr.io/google_containers/addon-resizer:1.6
                    name: heapster-nanny
                    resources:
                      limits:
                        cpu: 50m
                        memory: 90Mi
                      requests:
                        cpu: 50m
                        memory: 90Mi
                    env:
                      - name: MY_POD_NAME
                        valueFrom:
                          fieldRef:
                            fieldPath: metadata.name
                      - name: MY_POD_NAMESPACE
                        valueFrom:
                          fieldRef:
                            fieldPath: metadata.namespace
                    command:
                      - /pod_nanny
                      - --cpu=80m
                      - --extra-cpu=4m
                      - --memory=200Mi
                      - --extra-memory=4Mi
                      - --threshold=5
                      - --deployment=heapster-v1.2.0
                      - --container=heapster
                      - --poll-period=300000
                      - --estimator=exponential
    - path: /srv/kubernetes/manifests/heapster-svc.yaml
      filesystem: root
      contents:
        inline: |
          kind: Service
          apiVersion: v1
          metadata: 
            name: heapster
            namespace: kube-system
            labels: 
              kubernetes.io/cluster-service: "true"
              kubernetes.io/name: "Heapster"
          spec: 
            ports: 
              - port: 80
                targetPort: 8082
            selector: 
              k8s-app: heapster
    - path: /srv/kubernetes/manifests/kube-dashboard-deployment.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: extensions/v1beta1
          kind: Deployment
          metadata:
            name: kubernetes-dashboard
            namespace: kube-system
            labels:
              k8s-app: kubernetes-dashboard
              kubernetes.io/cluster-service: "true"
          spec:
            selector:
              matchLabels:
                k8s-app: kubernetes-dashboard
            template:
              metadata:
                labels:
                  k8s-app: kubernetes-dashboard
                annotations:
                  scheduler.alpha.kubernetes.io/critical-pod: ''
                  scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
              spec:
                containers:
                - name: kubernetes-dashboard
                  image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.0
                  resources:
                    # keep request = limit to keep this container in guaranteed class
                    limits:
                      cpu: 100m
                      memory: 50Mi
                    requests:
                      cpu: 100m
                      memory: 50Mi
                  ports:
                  - containerPort: 9090
                  livenessProbe:
                    httpGet:
                      path: /
                      port: 9090
                    initialDelaySeconds: 30
                    timeoutSeconds: 30
    - path: /srv/kubernetes/manifests/kube-dashboard-svc.yaml
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: Service
          metadata:
            name: kubernetes-dashboard
            namespace: kube-system
            labels:
              k8s-app: kubernetes-dashboard
              kubernetes.io/cluster-service: "true"
          spec:
            selector:
              k8s-app: kubernetes-dashboard
            ports:
            - port: 80
              targetPort: 9090
    {{ if eq .container_runtime "rkt" }}
    - path: /opt/bin/host-rkt
      filesystem: root
      mode: 0544
      contents:
        inline: |
          #!/bin/sh
          # This is bind mounted into the kubelet rootfs and all rkt shell-outs go
          # through this rkt wrapper. It essentially enters the host mount namespace
          # (which it is already in) only for the purpose of breaking out of the chroot
          # before calling rkt. It makes things like rkt gc work and avoids bind mounting
          # in certain rkt filesystem dependancies into the kubelet rootfs. This can
          # eventually be obviated when the write-api stuff gets upstream and rkt gc is
          # through the api-server. Related issue:
          # https://github.com/coreos/rkt/issues/2878
          exec nsenter -m -u -i -n -p -t 1 -- /usr/bin/rkt "$@"
    {{ end }}
    - path: /opt/k8s-addons
      filesystem: root
      mode: 0544
      contents:
        inline: |
          #!/bin/bash -ex
          echo "Waiting for Kubernetes API..."
          until curl --silent "http://127.0.0.1:8080/version"
          do
            sleep 5
          done
          echo "K8S: DNS addon"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/kube-dns-deployment.yaml)" "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/deployments"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/kube-dns-svc.yaml)" "http://127.0.0.1:8080/api/v1/namespaces/kube-system/services"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/kube-dns-autoscaler-deployment.yaml)" "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/deployments"
          echo "K8S: Heapster addon"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/heapster-deployment.yaml)" "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/deployments"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/heapster-svc.yaml)" "http://127.0.0.1:8080/api/v1/namespaces/kube-system/services"
          echo "K8S: Dashboard addon"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/kube-dashboard-deployment.yaml)" "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/deployments"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/kube-dashboard-svc.yaml)" "http://127.0.0.1:8080/api/v1/namespaces/kube-system/services"
          echo "K8S: Contiv addon"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/netplugin-crb.yaml)" "http://127.0.0.1:8080/apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindings"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/netplugin-cr.yaml)" "http://127.0.0.1:8080/apis/rbac.authorization.k8s.io/v1beta1/namespaces/kube-system/rolebindings"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/netplugin-sa.yaml)" "http://127.0.0.1:8080/api/v1/namespaces/kube-system/serviceaccounts"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/netmaster-crb.yaml)" "http://127.0.0.1:8080/apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindings"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/netmaster-cr.yaml)" "http://127.0.0.1:8080/apis/rbac.authorization.k8s.io/v1beta1/namespaces/kube-system/rolebindings"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/netmaster-sa.yaml)" "http://127.0.0.1:8080/api/v1/namespaces/kube-system/serviceaccounts"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/contiv-cm.yaml)" "http://127.0.0.1:8080/api/v1/namespaces/kube-system/configmaps"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/contiv-ds.yaml)" "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/daemonsets"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/netmaster-rs.yaml)" "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/replicasets"
          curl --silent -H "Content-Type: application/yaml" -XPOST -d"$(cat /srv/kubernetes/manifests/contiv-proxy-rs.yaml)" "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/replicasets"

{{ if index . "ssh_authorized_keys" }}
passwd:
  users:
    - name: core
      ssh_authorized_keys:
        {{ range $element := .ssh_authorized_keys }}
        - {{$element}}
        {{end}}
{{end}}
